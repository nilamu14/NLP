{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87d82f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the following libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLTK libraries\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Visualization libraries\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image # for world cloud image\n",
    "\n",
    "# Spacy for preprocessing\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "# To change date to datetime\n",
    "from datetime import datetime\n",
    "import re \n",
    "\n",
    "from collections import Counter\n",
    "import string\n",
    "import scipy.sparse\n",
    "\n",
    "# Gensim libraries\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import pyLDAvis.gensim_models\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import matutils\n",
    "\n",
    "# To show all the columns\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "\n",
    "# to pickle dataframe\n",
    "import pickle\n",
    "\n",
    "# Avoid warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Enable logging for gensim - optional but important\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d282aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4011cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Following code worked for me and I'm using Google Colaboratory.\n",
    "\n",
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f24d5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/gdrive/MyDrive/car_5_brands.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a33922",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa14b9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6701bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reset the index here ##\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e9693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unammed column and Author _name column\n",
    "df.drop(['car_year'],axis=1,inplace=True)\n",
    "# CHeck the data info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee66210",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7d004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for nun values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f9b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e619a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting the Vehicle_title into year, make and model column\n",
    "df['year'] = df.Vehicle_Title.str.split(' ').apply(lambda x:x[0])\n",
    "df['make'] = df.Vehicle_Title.str.split(' ').apply(lambda x:x[1])\n",
    "df['model'] = df.Vehicle_Title.str.split(' ').apply(lambda x:x[2])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb7c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_review_count = brand.groupby('make').count()['review'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38975ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exploratory Data Analysis ##\n",
    "\n",
    "# To see the percentage of each brands review in the dataset\n",
    "df_review_pct = df['brand_name'].value_counts(normalize = True).round(2) * 100 \n",
    "df_review_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd8082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using plotly to create Barchat\n",
    "bar_go = go.Bar(x = df_review_count['brand_name'], y = df_review_count['review'], name='Review count')\n",
    "fig = go.Figure(\n",
    "    data=[bar_go],\n",
    "    layout=go.Layout(width=1000, height=600, title='Brand Review Count', xaxis_title= 'Brand Name', yaxis_title='Review count'))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the count of each brand according to their rating\n",
    "grouped_brand = df.groupby([df.brand_name, df.Rating]).size().reset_index().rename(columns = {0: 'counts'})\n",
    "grouped_brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af98730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the numbers from the review\n",
    "df['review'] = df['review'].apply(lambda x: re.sub(r'[^A-Za-z\\s]', '', x))\n",
    "\n",
    "# Convert the reviews to lowercase\n",
    "df['review'] = df['review'].map(lambda x: x.lower())\n",
    "df.review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc34ad",
   "metadata": {},
   "outputs": [],
   "source": [
    " ##Join the review.\n",
    "long_string = ','.join(list(df.review.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af304304",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wordcloud ##\n",
    "long_string = ','.join(list(df.review.values))\n",
    "# Import the image of a car to have it as mask\n",
    "##car_mask = np.array(Image.open(\"audi_cloud.png\"))\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=100, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "\n",
    "plt.figure(figsize= (20,7))\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Common 100 words in reviews\", pad = 14, weight = 'bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5d392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.to_file(\"Car_reviw.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da69842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c1e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the stop-words\n",
    "stop_words = stopwords.words('english')\n",
    "# stop_words.extend(['])\n",
    "stop_words.extend(['porsche,' 'mercede','comfortsport', 'mercedes','mercedes-benz', 'honda','toyota','audi', 'benz','bentley','lexus',\n",
    "                  'nissan','volvo','drive','nt','like','vehicle','infiniti','good','miles','corvette','come','edmund','lotus','diego','snake',\n",
    "                 'porsche', 'cayman','bought','year','minute','chicago','car','home', 'work','think','suv','people','edmunds',\n",
    "                  'cabriolet','lexuss','japan','husband','baby','range', 'rover','cadillac','cadillacs','michelin','texas','second',\n",
    "                   'awsome','one','now', 'take', 'give', 'new','levinson','road','love','sedan','wife','sport','bang','tank',\n",
    "                   'truck','lemon','imho','pathfinder','infinity','convertible','allroad','conv','bike','ski','grocery','mclass'\n",
    "                  ,'hardtop','club','hubby','child','zoom','test','etc','brain','ashamed','carmax','alpina','rocketship','great','germany',\n",
    "                  'autobahn','mercedez'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faa882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematized_review(text): # text\n",
    "    rev_text = nlp(text)\n",
    "    # Extract lematized words in lower case format if not digits, not punctuation, not stopword, and length not less than 2\n",
    "    rev_text = ([token.lemma_.lower() for token in rev_text if not token.is_stop and token.text not in stop_words and not token.is_punct and len(token.text) > 3])\n",
    "    return rev_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Applying the function on the reviews \n",
    "\n",
    "df['review'] = df['review'].apply(lematized_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61e9646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Let's pickle it for later use\n",
    "clean_brand_review = df['review']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e8cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create Dictionary\n",
    "id2word_1 = corpora.Dictionary(clean_brand_review)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus_1 = [id2word_1.doc2bow(review) for review in clean_brand_review]\n",
    "\n",
    " # Build LDA model\n",
    "ldamodel = LdaMulticore(corpus= corpus_1, num_topics =8, id2word=id2word_1,chunksize=2000, passes=50,per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e9ecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(ldamodel.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202784a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "\n",
    "#It's a measure of how good the model is. The lower the better. Perplexity is a negative value\n",
    "print('\\nPerplexity: ', ldamodel.log_perplexity(corpus_1))  \n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel, texts=clean_brand_review, dictionary=id2word_1, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\n Basic Ldamodel Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff60cf",
   "metadata": {},
   "source": [
    "Perplexity: -7.703112835107014\n",
    "\n",
    "Basic Ldamodel Coherence Score: 0.5423936154526896 Notes\n",
    "\n",
    "perplexity is a measurement of how well a probability distribution or probability model predicts a sample. It may be used to compare probability models. A low perplexity indicates the probability distribution is good at predicting the sample.\n",
    "\n",
    "The coherence score is used in assessing the quality of the learned topics, the closer to 1 the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98424c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models.wrappers.ldamallet import malletmodel2ldamodel\n",
    "os.environ['MALLET_HOME'] = '/content/gdrive/MyDrive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d694f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#  point the path to the mallet path on my computer\n",
    "mallet_path = '/content/'#insert the path\n",
    "\n",
    "# Instantiate\n",
    "ldamallet = LdaMallet(mallet_path,corpus=corpus_1, num_topics=10, id2word=id2word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775938ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Topics\n",
    "from pprint import pprint\n",
    "pprint(ldamallet.show_topics(formatted=False))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=clean_brand_review, dictionary=id2word_1, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\n Mallet Coherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60096c0",
   "metadata": {},
   "source": [
    "To find the optimum number of topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c712aa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to get coherence score\n",
    "def my_coherence_vals(dictionary, corpus, texts, limit, start, step):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LdaMallet(mallet_path, corpus=corpus_1, num_topics=num_topics, id2word=id2word_1)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5b5160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the coherence values\n",
    "model_list, coherence_values = my_coherence_vals(dictionary=id2word_1, corpus=corpus_1, \n",
    "                                                 texts=clean_brand_review, start=2, limit=26, step=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06526f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph for the coherence value scores vs number of topics\n",
    "limit=26; start=2; step=6;\n",
    "topics = range(start, limit, step)\n",
    "plt.plot(topics, coherence_values)\n",
    "plt.title(\"Coherence value score with the number of topics\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae929c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for best, cv in zip(topics, coherence_values):\n",
    "    print(\"Topic \", best, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6994f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the best topics\n",
    "optimal_model = model_list[1]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c29eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the best topics\n",
    "optimal_model = model_list[1]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1216f57",
   "metadata": {},
   "source": [
    "Document Topic in each review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3777ab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sentence topics\n",
    "def sentence_topics(ldamodel=ldamodel, corpus=corpus_1, texts=clean_brand_review):\n",
    "    # Init output\n",
    "    topics_df = pd.DataFrame()\n",
    "\n",
    "    # Looping through the documents to find the main topics\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "        \n",
    "        # look for the Dominant topic, % contribution and Keywords \n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            \n",
    "            # Diplay the dominant topics\n",
    "            if j == 0:  \n",
    "                dom = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in dom])\n",
    "                topics_df = topics_df.append(pd.Series([int(topic_num), round(prop_topic,2)*100, topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    topics_df.columns = ['Dominant_Review_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Concatenate the text and the topics_df\n",
    "    contents = pd.Series(texts)\n",
    "    topics_df = pd.concat([topics_df, contents], axis=1)\n",
    "    return(topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = sentence_topics(ldamodel=ldamodel, corpus=corpus_1, texts=clean_brand_review)\n",
    "\n",
    "# Format\n",
    "dominant_review_topic = df_topic_sents_keywords.reset_index()\n",
    "dominant_review_topic.columns = ['Review_No', 'Dominant_Review_Topic', 'Percent_contr_per_topic', 'Review_Keywords', 'Original review']\n",
    "\n",
    "# Show\n",
    "dominant_review_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f9d08b",
   "metadata": {},
   "source": [
    "Here I will investigate the percentage of most document in each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89df5694",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\NILANJ~1\\AppData\\Local\\Temp/ipykernel_9936/3699446853.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# The Dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msent_topics_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtopics_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_topic_sents_keywords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Dominant_Review_Topic'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# The Dataframe\n",
    "sent_topics_df = pd.DataFrame()\n",
    "\n",
    "topics_out = df_topic_sents_keywords.groupby('Dominant_Review_Topic')\n",
    "\n",
    "for i, j in topics_out:\n",
    "    sent_topics_df = pd.concat([sent_topics_df,j.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], axis=0)\n",
    "\n",
    "    \n",
    "sent_topics_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_df.columns = ['Topic_Num', \"Percent_contr_per_topic\", \"Review_Keywords\", \"Original review\"]\n",
    "\n",
    "# Display the 8 topics\n",
    "sent_topics_df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e4f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the review of each brand in a DataFrame\n",
    "audi_df = ([sent for sent in df.loc[df['make'] == 'Audi', 'review']])\n",
    "bmw_df = ([sent for sent in df.loc[df['make'] == 'BMW', 'review']])\n",
    "mercedes_df = ([sent for sent in df.loc[df['make'] == 'Mercedes-Benz', 'review']])\n",
    "lexus_df = ([sent for sent in df.loc[df['make'] == 'Lexus', 'review']])\n",
    "inifiniti_df = ([sent for sent in df.loc[df['make'] == 'INFINITI', 'review']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b38257",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Defining a function to get the topics and visualize them \n",
    "def each_brand(text):\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word_2 = corpora.Dictionary(text)\n",
    "\n",
    "    # Create Corpus: Term Document Frequency\n",
    "    corpus_2 = [id2word_2.doc2bow(review) for review in text]\n",
    "    \n",
    "    # Here I decided to reduce the number of topics to only six for each brand\n",
    "    model = LdaMulticore(corpus=corpus_2, num_topics = 6, id2word=id2word_2,chunksize=2000, passes=80,per_word_topics=True)\n",
    "    \n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(model, corpus=corpus_2, dictionary=id2word_2,sort_topics=False)\n",
    "\n",
    "    return LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef68362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the LDA model on each brand to visual the topics\n",
    "Audi_lda = each_brand(audi_df)\n",
    "lexus_lda = each_brand(lexus_df)\n",
    "bmw_lda = each_brand(bmw_df)\n",
    "mercedes_lda = each_brand(mercedes_df)\n",
    "inifiniti_lda = each_brand(inifiniti_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85215cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
